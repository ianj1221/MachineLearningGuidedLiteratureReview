{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/sandbox_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-08 13:16:07.732148: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import transformers\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer\n",
    "from keras_tuner import HyperModel\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from bs4 import BeautifulSoup    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create definitions to preprocess the text and model the relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove HTML tags\n",
    "def remove_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "# Remove stopwords, punctuation, and convert to lowercase\n",
    "def preprocess_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = remove_html_tags(text)\n",
    "    \n",
    "    # Replace hyphens between words with a space\n",
    "    text = text.replace('-', ' ')\n",
    "    \n",
    "    # Tokenize text into words\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and punctuation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punctuation = set(string.punctuation)\n",
    "    tokens = [word.lower() for word in tokens if word.lower() not in stop_words and word not in punctuation]\n",
    "    \n",
    "    # Lemmatize words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Join tokens back into a single string\n",
    "    processed_text = ' '.join(tokens)\n",
    "    \n",
    "    return processed_text\n",
    "# Load the BERT tokenizer\n",
    "max_length = 256\n",
    "vocab_size = 30522\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "def tokenize_and_batch(train, train_labels, tokenizer, batch_size, shuffle_buffer_size,max_length):\n",
    "    # Tokenize the train data\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for text in train:\n",
    "        train_tokens = tokenizer.encode_plus(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            #return_tensors='tf'\n",
    "            )\n",
    "        input_ids.append(train_tokens['input_ids'])\n",
    "        attention_masks.append(train_tokens['attention_mask'])\n",
    "    #print(np.array(input_ids[:5]))\n",
    "    # Convert the tokens into tensors\n",
    "    tensor = tf.data.Dataset.from_tensor_slices((input_ids,train_labels))\n",
    " \n",
    "    # # Shuffle and Batch the tensors\n",
    "    tensor = tensor.shuffle(shuffle_buffer_size)\n",
    "    tensor = tensor.batch(batch_size)\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "class MyHyperModel(HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = keras.Sequential()\n",
    "        model.add(layers.Input(shape=(max_length,), dtype='int32'))\n",
    "        model.add(layers.Embedding(input_dim=vocab_size, \n",
    "                                   output_dim=hp.Int('embed_dim', min_value=4, max_value=128),\n",
    "                                   mask_zero=True))\n",
    "        #try flatten or move this to the end\n",
    "        model.add(layers.GlobalAveragePooling1D())\n",
    "        # Add 1 to 3 dense layers\n",
    "        for i in range(hp.Int('num_dense_layers', min_value=1, max_value=3)):\n",
    "            model.add(layers.Dense(units=hp.Int('units_' + str(i), min_value=32, max_value=512),\n",
    "                                    activation=hp.Choice('activation_' + str(i), values=['sigmoid', 'relu', 'tanh'])))\n",
    "            \n",
    "            # Add dropout layer between dense layers\n",
    "            if hp.Boolean('dropout_' + str(i)):\n",
    "                model.add(layers.Dropout(rate=0.2))\n",
    "        \n",
    "        # Add final dense layer with 1 node\n",
    "        \n",
    "        model.add(layers.Dense(units=1))\n",
    "        learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "        # Compile the model\n",
    "        model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                      loss= tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                      metrics= tf.metrics.BinaryAccuracy(threshold=0.0)\n",
    "                      )\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l_/ltt5y4mj2y9_mx7w26zlssfw0000gn/T/ipykernel_83730/3050509258.py:3: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    }
   ],
   "source": [
    "#######################User Input############################\n",
    "round = 1\n",
    "df = pd.read_csv('TACScrapedCleanMSDOS.csv', encoding='latin')\n",
    "#############################################################\n",
    "df['clean_TA'] = df['title'].fillna('').apply(preprocess_text) + ' ' + df['Abstracts'].fillna('').apply(preprocess_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 50 Complete [00h 00m 08s]\n",
      "val_binary_accuracy: 0.9285714030265808\n",
      "\n",
      "Best val_binary_accuracy So Far: 1.0\n",
      "Total elapsed time: 00h 08m 13s\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing to the 'data_TA' column\n",
    "data_TA = df.dropna(subset=['Relevant'])\n",
    "#print(data_TA['Relevant'].isna().sum())\n",
    "if data_TA.loc[data_TA['Relevant'] == 5].shape[0]<data_TA.loc[data_TA['Relevant'] == 1].shape[0]:\n",
    "     sample_size = data_TA.loc[data_TA['Relevant'] == 5].shape[0]\n",
    "     print('Less relevant docs')\n",
    "else:\n",
    "     sample_size = data_TA.loc[data_TA['Relevant'] == 1].shape[0] \n",
    "     print('Less nonrelevant docs')  \n",
    "\n",
    "data_TA.loc[data_TA['Relevant']==1, 'Relevant'] = 0\n",
    "data_TA.loc[data_TA['Relevant']==5, 'Relevant'] = 1\n",
    "\n",
    "# Create a list of 10 random numbers from 1 through 100\n",
    "random_numbers = [15, 32, 69, 75, 83, 8, 68, 99, 44, 92\n",
    "                  ] #random.sample(range(1, 101), 10)\n",
    "\n",
    "# Iterate through the list of random numbers and create a training and validation set for each\n",
    "\n",
    "for num in random_numbers:\n",
    "    # Select 33 rows where the 'relevant' column is 5\n",
    "    relevant_1_rows = data_TA[data_TA['Relevant'] == 1].sample(n=sample_size, random_state=num)\n",
    "    \n",
    "    # Select 33 rows where the 'relevant' column is 1\n",
    "    relevant_0_rows = data_TA[data_TA['Relevant'] == 0].sample(n=sample_size, random_state=num)\n",
    "    balanced_data = pd.concat([relevant_0_rows, relevant_1_rows]).sample(frac=1,random_state=num).reset_index(drop=True)\n",
    "\n",
    "    # Split off 10% for a validation set\n",
    "    \n",
    "    valnotrel = balanced_data.loc[balanced_data['Relevant'] == 0].sample(frac=0.2, random_state=num)\n",
    "    valrel = balanced_data.loc[balanced_data['Relevant'] == 1].sample(frac=0.2, random_state=num)\n",
    "    raw_val = pd.concat([valnotrel, valrel]).sample(frac=1,random_state=num)\n",
    "    \n",
    "    raw_train = balanced_data.drop(raw_val.index).reset_index(drop=True)\n",
    "\n",
    "# Save the training and validation sets to CSV\n",
    "    raw_train[['Relevant','clean_TA']].to_csv(f'data/train_{num}.csv', index=False)\n",
    "    raw_val[['Relevant','clean_TA']].to_csv(f'data/val_{num}.csv', index=False)\n",
    "\n",
    "# reload the data  \n",
    "    #raw_train = pd.read_csv(f'train_{num}.csv')\n",
    "    #raw_val = pd.read_csv(f'val_{num}.csv')\n",
    "\n",
    "#tokenize the data\n",
    "\n",
    "    train_data = tokenize_and_batch(raw_train['clean_TA'], \n",
    "                                    raw_train['Relevant'], \n",
    "                                    tokenizer, \n",
    "                                    batch_size=10, \n",
    "                                    shuffle_buffer_size=10000,\n",
    "                                    max_length=256)\n",
    "    #print(np.asarray(train_data))\n",
    "    val_data = tokenize_and_batch(raw_val['clean_TA'], \n",
    "                                  raw_val['Relevant'], \n",
    "                                  tokenizer=tokenizer, \n",
    "                                  batch_size=10, \n",
    "                                  shuffle_buffer_size=10000,\n",
    "                                  max_length=256)\n",
    "#set up and train the model\n",
    "    best_model = []\n",
    "    hypermodel = MyHyperModel()\n",
    "\n",
    "    tuner = RandomSearch(\n",
    "        hypermodel,\n",
    "        objective='val_binary_accuracy',\n",
    "        max_trials=50,\n",
    "        directory='Tuner_delete_me',\n",
    "        project_name=f'R1_{num}'\n",
    "    )\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        patience=5, \n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.75,\n",
    "        patience=3,\n",
    "        min_lr=1e-7\n",
    "    )\n",
    "    \n",
    "    tuner.search(train_data,\n",
    "                 epochs=50, \n",
    "                 validation_data=(val_data\n",
    "                                  ),\n",
    "                 batch_size=10,\n",
    "                 callbacks=[early_stopping,reduce_lr]\n",
    "                 )\n",
    "    \n",
    "    best_model = tuner.get_best_models(num_models=10)\n",
    "    for i in range(len(best_model)):\n",
    "         best_model[0][i].save(f'models/R{round}_model_{num}_{i}.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data tokenized using tokenizer BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "394/394 [==============================] - 3s 6ms/step\n",
      "394/394 [==============================] - 3s 7ms/step\n",
      "394/394 [==============================] - 3s 8ms/step\n",
      "394/394 [==============================] - 4s 9ms/step\n",
      "394/394 [==============================] - 3s 8ms/step\n",
      "394/394 [==============================] - 3s 8ms/step\n",
      "394/394 [==============================] - 4s 9ms/step\n",
      "394/394 [==============================] - 6s 13ms/step\n",
      "394/394 [==============================] - 6s 13ms/step\n",
      "394/394 [==============================] - 6s 13ms/step\n",
      "394/394 [==============================] - 6s 13ms/step\n",
      "394/394 [==============================] - 3s 8ms/step\n",
      "394/394 [==============================] - 5s 11ms/step\n",
      "394/394 [==============================] - 4s 10ms/step\n",
      "394/394 [==============================] - 3s 6ms/step\n",
      "394/394 [==============================] - 4s 9ms/step\n",
      "394/394 [==============================] - 5s 12ms/step\n",
      "394/394 [==============================] - 7s 14ms/step\n",
      "394/394 [==============================] - 4s 7ms/step\n",
      "394/394 [==============================] - 3s 7ms/step\n",
      "394/394 [==============================] - 3s 7ms/step\n",
      "394/394 [==============================] - 4s 9ms/step\n",
      "394/394 [==============================] - 4s 9ms/step\n",
      "394/394 [==============================] - 4s 8ms/step\n",
      "394/394 [==============================] - 3s 7ms/step\n",
      "394/394 [==============================] - 5s 9ms/step\n",
      "394/394 [==============================] - 4s 11ms/step\n",
      "394/394 [==============================] - 3s 7ms/step\n",
      "394/394 [==============================] - 4s 11ms/step\n",
      "394/394 [==============================] - 4s 9ms/step\n",
      "[[ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " ...\n",
      " [30]\n",
      " [30]\n",
      " [30]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Master patent number</th>\n",
       "      <th>title</th>\n",
       "      <th>assignee</th>\n",
       "      <th>inventor/author</th>\n",
       "      <th>priority date</th>\n",
       "      <th>filing/creation date</th>\n",
       "      <th>publication date</th>\n",
       "      <th>grant date</th>\n",
       "      <th>result link</th>\n",
       "      <th>representative figure link</th>\n",
       "      <th>Abstracts</th>\n",
       "      <th>Relevant</th>\n",
       "      <th>clean_TA</th>\n",
       "      <th>Uncertanty</th>\n",
       "      <th>Prediction score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>CN-113956744-A</td>\n",
       "      <td>Low-odor rust-coated single-component epoxy co...</td>\n",
       "      <td>_ññ_Ø_ïà_ó_ÿ__µ_Ô_óØ_¦_¶_, _ïà_ó_...</td>\n",
       "      <td>__ë___, _óÑ­__, _ö__</td>\n",
       "      <td>10/26/21</td>\n",
       "      <td>10/26/21</td>\n",
       "      <td>1/21/22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://patents.google.com/patent/CN113956744A/en</td>\n",
       "      <td>https://patentimages.storage.googleapis.com/6c...</td>\n",
       "      <td>The invention discloses a low-odor rust-coated...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>low odor rust coated single component epoxy co...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>JP-2003531266-A</td>\n",
       "      <td>Pigment preparations and waterborne basecoats ...</td>\n",
       "      <td>_____Ô_½___Ô__ï____Ô____Ô_×_...</td>\n",
       "      <td>_Ø__ _ï _______, _½_____¯  _...</td>\n",
       "      <td>4/19/00</td>\n",
       "      <td>4/18/01</td>\n",
       "      <td>10/21/03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://patents.google.com/patent/JP2003531266...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r\\r \\r  (57)_ê_®Æ_êâ\\r_í¬__½_ÿ_×__â___...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pigment preparation waterborne basecoats prepa...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>JP-2004532288-A</td>\n",
       "      <td>Room temperature curing fast drying solvent-co...</td>\n",
       "      <td>_____Ô_½___Ô__ï____Ô____Ô_×_...</td>\n",
       "      <td>_____ ___ê__, _ª_____________¸_...</td>\n",
       "      <td>2/6/01</td>\n",
       "      <td>2/6/02</td>\n",
       "      <td>10/21/04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://patents.google.com/patent/JP2004532288...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r\\r óØ¶¦¬óö___ê¯_¾Ô__ù_µ¥__óí·_à...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>room temperature curing fast drying solvent co...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306</th>\n",
       "      <td>KR-100948993-B1</td>\n",
       "      <td>Method for forming multiple coating film in au...</td>\n",
       "      <td>_____×_¶ __ê_®_àª_àª</td>\n",
       "      <td>_ÏÆ_®_ÏÆ, _____Ï×, _ê__í</td>\n",
       "      <td>2/13/03</td>\n",
       "      <td>2/13/03</td>\n",
       "      <td>3/23/10</td>\n",
       "      <td>3/23/10</td>\n",
       "      <td>https://patents.google.com/patent/KR100948993B...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r\\r __ _­Ø_¯_ê ___¦_­ª_ö __æ_æ _Æ_Ï ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>method forming multiple coating film automobil...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1831</th>\n",
       "      <td>KR-101194454-B1</td>\n",
       "      <td>Epoxy coating composition</td>\n",
       "      <td>_Æ_ö ­ê­ê_ _Ò_ö_æ ­ê_Ýê_Ø__­ê_®__</td>\n",
       "      <td>_Ï_¶__ __ö_­à, __®___¾_ _¶__®, ___¶_ö_ ...</td>\n",
       "      <td>3/24/05</td>\n",
       "      <td>3/24/05</td>\n",
       "      <td>10/24/12</td>\n",
       "      <td>10/24/12</td>\n",
       "      <td>https://patents.google.com/patent/KR101194454B...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r\\r\\r ___ÒØassignment\\r\\r\\r\\r _Æ_¶ _Ø_ö ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>epoxy coating composition ___òøassignment _æ...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Master patent number                                              title  \\\n",
       "483        CN-113956744-A  Low-odor rust-coated single-component epoxy co...   \n",
       "803       JP-2003531266-A  Pigment preparations and waterborne basecoats ...   \n",
       "628       JP-2004532288-A  Room temperature curing fast drying solvent-co...   \n",
       "1306      KR-100948993-B1  Method for forming multiple coating film in au...   \n",
       "1831      KR-101194454-B1                         Epoxy coating composition    \n",
       "\n",
       "                                               assignee  \\\n",
       "483   _ññ_Ø_ïà_ó_ÿ__µ_Ô_óØ_¦_¶_, _ïà_ó_...   \n",
       "803   _____Ô_½___Ô_\n",
       "_ï____Ô____Ô_×_...   \n",
       "628   _____Ô_½___Ô_\n",
       "_ï____Ô____Ô_×_...   \n",
       "1306                          _____×_¶ __ê_®_àª_àª   \n",
       "1831   _Æ_ö ­ê­ê_ _Ò_ö_æ ­ê_Ýê_Ø__­ê_®__   \n",
       "\n",
       "                                        inventor/author priority date  \\\n",
       "483                        __ë___, _óÑ­__, _ö__      10/26/21   \n",
       "803   _Ø__ _ï _______, _½_____¯  _...       4/19/00   \n",
       "628   _____ ___ê__, _ª_____________¸_...        2/6/01   \n",
       "1306                    _ÏÆ_®_ÏÆ, _____Ï×, _ê__í       2/13/03   \n",
       "1831  _Ï_¶__ __ö_­à, __®___¾_ _¶__®, ___¶_ö_ ...       3/24/05   \n",
       "\n",
       "     filing/creation date publication date grant date  \\\n",
       "483              10/26/21          1/21/22        NaN   \n",
       "803               4/18/01         10/21/03        NaN   \n",
       "628                2/6/02         10/21/04        NaN   \n",
       "1306              2/13/03          3/23/10    3/23/10   \n",
       "1831              3/24/05         10/24/12   10/24/12   \n",
       "\n",
       "                                            result link  \\\n",
       "483   https://patents.google.com/patent/CN113956744A/en   \n",
       "803   https://patents.google.com/patent/JP2003531266...   \n",
       "628   https://patents.google.com/patent/JP2004532288...   \n",
       "1306  https://patents.google.com/patent/KR100948993B...   \n",
       "1831  https://patents.google.com/patent/KR101194454B...   \n",
       "\n",
       "                             representative figure link  \\\n",
       "483   https://patentimages.storage.googleapis.com/6c...   \n",
       "803                                                 NaN   \n",
       "628                                                 NaN   \n",
       "1306                                                NaN   \n",
       "1831                                                NaN   \n",
       "\n",
       "                                              Abstracts  Relevant  \\\n",
       "483   The invention discloses a low-odor rust-coated...       5.0   \n",
       "803   \\r\\r \\r  (57)_ê_®Æ_êâ\\r_í¬__½_ÿ_×__â___...       NaN   \n",
       "628   \\r\\r óØ¶¦¬óö___ê\n",
       "¯_¾Ô__ù_µ¥__óí·_à...       NaN   \n",
       "1306  \\r\\r __ _­Ø_¯_ê ___¦_­ª_ö __æ_æ _Æ_Ï ...       NaN   \n",
       "1831  \\r\\r\\r ___ÒØassignment\\r\\r\\r\\r _Æ_¶ _Ø_ö ...       NaN   \n",
       "\n",
       "                                               clean_TA  Uncertanty  \\\n",
       "483   low odor rust coated single component epoxy co...    0.033333   \n",
       "803   pigment preparation waterborne basecoats prepa...    0.033333   \n",
       "628   room temperature curing fast drying solvent co...    0.033333   \n",
       "1306  method forming multiple coating film automobil...    0.033333   \n",
       "1831  epoxy coating composition ___òøassignment _æ...    0.033333   \n",
       "\n",
       "      Prediction score  \n",
       "483                 30  \n",
       "803                 30  \n",
       "628                 30  \n",
       "1306                30  \n",
       "1831                30  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict df using all 100 models\n",
    "random_numbers = [15, 32, 69, \n",
    "                  75, 83, 8, 68, 99, 44, 92\n",
    "                  ]\n",
    "\n",
    "predictions = []\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "targets = df['Relevant'].values\n",
    "for text in df['clean_TA']:\n",
    "    train_tokens = tokenizer.encode_plus(\n",
    "        text,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        #return_tensors='tf'\n",
    "        )\n",
    "    input_ids.append(train_tokens['input_ids'])\n",
    "    attention_masks.append(train_tokens['attention_mask'])\n",
    "features = np.array(input_ids)\n",
    "print('All data tokenized using tokenizer', tokenizer)\n",
    "df2=pd.DataFrame()\n",
    "for num in random_numbers:\n",
    "    for i in range(10):\n",
    "        model = keras.models.load_model(f'models/R{round}_model_{num}_{i}.keras')\n",
    "        predict = model.predict(features,batch_size=10)\n",
    "        predict = np.where(predict >= 0, 1, 0)\n",
    "        predictions.append(predict)\n",
    "\n",
    "sum_predictions = np.sum(predictions, axis=0)\n",
    "#calculate uncertainty as 1 divided by the sum of 1 predictions mininus the sum of 0 predictions\n",
    "#count of 0's in predictions\n",
    "count_0 = np.count_nonzero(np.array(predictions) == 0, axis=0)\n",
    "print(count_0)\n",
    "df[f'R{round} Uncertanty'] = 1 / (np.abs(sum_predictions - count_0)+10**(-6))\n",
    "df[f'R{round} Prediction score'] = sum_predictions\n",
    "df = df.sort_values(by=f'R{round} Prediction score', ascending=False)\n",
    "df.to_excel(f'R{round}_predictions_for_review.xlsx', index=False)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandbox_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
