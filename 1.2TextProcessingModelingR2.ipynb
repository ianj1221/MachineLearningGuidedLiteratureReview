{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/sandbox_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-06 07:33:02.562392: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import transformers\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer\n",
    "from keras_tuner import HyperModel\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from bs4 import BeautifulSoup    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create definitions to preprocess the text and model the relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove HTML tags\n",
    "def remove_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "# Remove stopwords, punctuation, and convert to lowercase\n",
    "def preprocess_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = remove_html_tags(text)\n",
    "    \n",
    "    # Replace hyphens between words with a space\n",
    "    text = text.replace('-', ' ')\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Tokenize text into words\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and punctuation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punctuation = set(string.punctuation)\n",
    "    tokens = [word.lower() for word in tokens if word.lower() not in stop_words and word not in punctuation]\n",
    "    \n",
    "    # Lemmatize words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Join tokens back into a single string\n",
    "    processed_text = ' '.join(tokens)\n",
    "    \n",
    "    return processed_text\n",
    "# Load the BERT tokenizer\n",
    "max_length = 256\n",
    "vocab_size = 30522\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "def tokenize_and_batch(train, train_labels, tokenizer, batch_size, shuffle_buffer_size,max_length):\n",
    "    # Tokenize the train data\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for text in train:\n",
    "        train_tokens = tokenizer.encode_plus(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            #return_tensors='tf'\n",
    "            )\n",
    "        input_ids.append(train_tokens['input_ids'])\n",
    "        attention_masks.append(train_tokens['attention_mask'])\n",
    "    #print(np.array(input_ids[:5]))\n",
    "    # Convert the tokens into tensors\n",
    "    tensor = tf.data.Dataset.from_tensor_slices((input_ids,train_labels))\n",
    " \n",
    "    # # Shuffle and Batch the tensors\n",
    "    tensor = tensor.shuffle(shuffle_buffer_size)\n",
    "    tensor = tensor.batch(batch_size)\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "class MyHyperModel(HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = keras.Sequential()\n",
    "        model.add(layers.Input(shape=(max_length,), dtype='int32'))\n",
    "        model.add(layers.Embedding(input_dim=vocab_size, \n",
    "                                   output_dim=hp.Int('embed_dim', min_value=4, max_value=128),\n",
    "                                   mask_zero=True))\n",
    "        #try flatten or move this to the end\n",
    "        model.add(layers.GlobalAveragePooling1D())\n",
    "        # Add 1 to 3 dense layers\n",
    "        for i in range(hp.Int('num_dense_layers', min_value=1, max_value=3)):\n",
    "            model.add(layers.Dense(units=hp.Int('units_' + str(i), min_value=32, max_value=512),\n",
    "                                    activation=hp.Choice('activation_' + str(i), values=['sigmoid', 'relu', 'tanh'])))\n",
    "            \n",
    "            # Add dropout layer between dense layers\n",
    "            if hp.Boolean('dropout_' + str(i)):\n",
    "                model.add(layers.Dropout(rate=0.2))\n",
    "        \n",
    "        # Add final dense layer with 1 node\n",
    "        \n",
    "        model.add(layers.Dense(units=1))\n",
    "        learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "        # Compile the model\n",
    "        model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                      loss= tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                      metrics= tf.metrics.BinaryAccuracy(threshold=0.0)\n",
    "                      )\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the 500 models and save the best 100\n",
    "### Our sampling strategy involves taking all the relevant docuents and pairing an equal number of randomly sampled non-relevant documents, we repeat this for 10 total datasets using 10 different random stattes to ensure a diffent mix in each traing set. 20% is then split off for validation back-propigation during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the current training round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "round = 2\n",
    "df = pd.read_excel(f'R{round}_review_for_retraining.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess the text if needed\n",
    "df['clean_TA'] = df['title'].fillna('').apply(preprocess_text) + ' ' + df['Abstracts'].fillna('').apply(preprocess_text) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 50 Complete [00h 00m 07s]\n",
      "val_binary_accuracy: 0.890625\n",
      "\n",
      "Best val_binary_accuracy So Far: 0.921875\n",
      "Total elapsed time: 00h 14m 30s\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing to the 'data_TA' column\n",
    "data_TA = df.dropna(subset=['Relevant'])\n",
    "\n",
    "if data_TA.loc[data_TA['Relevant'] == 5].shape[0]<data_TA.loc[data_TA['Relevant'] == 1].shape[0]:\n",
    "     sample_size = data_TA.loc[data_TA['Relevant'] == 5].shape[0]\n",
    "     print('Less relevant docs', sample_size)\n",
    "else:\n",
    "     sample_size = data_TA.loc[data_TA['Relevant'] == 1].shape[0] \n",
    "     print('Less nonrelevant docs', sample_size)  \n",
    "\n",
    "data_TA.loc[data_TA['Relevant']==1, 'Relevant'] = 0\n",
    "data_TA.loc[data_TA['Relevant']==5, 'Relevant'] = 1\n",
    "\n",
    "# Create a list of 10 random numbers from 1 through 100\n",
    "random_numbers = [15, 32, 69, 75, 83, 8, 68, 99, 44, 92\n",
    "                  ] #random.sample(range(1, 101), 10)\n",
    "\n",
    "# Iterate through the list of random numbers and create a training and validation set for each\n",
    "\n",
    "for num in random_numbers:\n",
    "    # Select sample_size rows where the 'relevant' column is 1\n",
    "    relevant_1_rows = data_TA[data_TA['Relevant'] == 1].sample(n=sample_size, random_state=num)\n",
    "    \n",
    "    # Select sample_size rows where the 'relevant' column is 0\n",
    "    relevant_0_rows = data_TA[data_TA['Relevant'] == 0].sample(n=sample_size, random_state=num)\n",
    "    balanced_data = pd.concat([relevant_0_rows, relevant_1_rows]).sample(frac=1,random_state=num).reset_index(drop=True)\n",
    "\n",
    "    # Split off 10% for a validation set\n",
    "    \n",
    "    valnotrel = balanced_data.loc[balanced_data['Relevant'] == 0].sample(frac=0.2, random_state=num)\n",
    "    valrel = balanced_data.loc[balanced_data['Relevant'] == 1].sample(frac=0.2, random_state=num)\n",
    "    raw_val = pd.concat([valnotrel, valrel]).sample(frac=1,random_state=num)\n",
    "    \n",
    "    raw_train = balanced_data.drop(raw_val.index).reset_index(drop=True)\n",
    "\n",
    "# Save the training and validation sets to CSV\n",
    "    raw_train[['Relevant','clean_TA']].to_csv(f'data/train__R{round}_{num}.csv', index=False)\n",
    "    raw_val[['Relevant','clean_TA']].to_csv(f'data/val_R{round}_{num}.csv', index=False)\n",
    "\n",
    "# reload the data  \n",
    "    #raw_train = pd.read_csv(f'train_{num}.csv')\n",
    "    #raw_val = pd.read_csv(f'val_{num}.csv')\n",
    "#for num in random_numbers:\n",
    "#tokenize the data\n",
    "\n",
    "    train_data = tokenize_and_batch(raw_train['clean_TA'], \n",
    "                                    raw_train['Relevant'], \n",
    "                                    tokenizer, \n",
    "                                    batch_size=10, \n",
    "                                    shuffle_buffer_size=10000,\n",
    "                                    max_length=256)\n",
    "    #print(np.asarray(train_data))\n",
    "    val_data = tokenize_and_batch(raw_val['clean_TA'], \n",
    "                                  raw_val['Relevant'], \n",
    "                                  tokenizer=tokenizer, \n",
    "                                  batch_size=10, \n",
    "                                  shuffle_buffer_size=10000,\n",
    "                                  max_length=256)\n",
    "#set up and train the model\n",
    "    best_model = []\n",
    "    hypermodel = MyHyperModel()\n",
    "\n",
    "    tuner = RandomSearch(\n",
    "        hypermodel,\n",
    "        objective='val_binary_accuracy',\n",
    "        max_trials=50,\n",
    "        directory='Tuner_delete_me',\n",
    "        project_name=f'R{round}_{num}'\n",
    "    )\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        patience=5, \n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.75,\n",
    "        patience=3,\n",
    "        min_lr=1e-7\n",
    "    )\n",
    "    \n",
    "    tuner.search(train_data,\n",
    "                 epochs=50, \n",
    "                 validation_data=(val_data\n",
    "                                  ),\n",
    "                 batch_size=10,\n",
    "                 callbacks=[early_stopping,reduce_lr]\n",
    "                 )\n",
    "    \n",
    "    best_model = tuner.get_best_models(num_models=10)\n",
    "    for i in range(len(best_model)):\n",
    "         best_model[i].save(f'models/R{round}_model_{num}_{i}.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict all of the articles using 100 models and sort the articles by decreasing concensus\n",
    "### this process will pull more relevant articles to the top of the list for review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Master patent number</th>\n",
       "      <th>title</th>\n",
       "      <th>assignee</th>\n",
       "      <th>inventor/author</th>\n",
       "      <th>priority date</th>\n",
       "      <th>filing/creation date</th>\n",
       "      <th>publication date</th>\n",
       "      <th>grant date</th>\n",
       "      <th>result link</th>\n",
       "      <th>representative figure link</th>\n",
       "      <th>Abstracts</th>\n",
       "      <th>Relevant</th>\n",
       "      <th>Round</th>\n",
       "      <th>clean_TA</th>\n",
       "      <th>Uncertanty</th>\n",
       "      <th>Prediction score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>CN-111925718-A</td>\n",
       "      <td>Universal primer for nonferrous metal and prep...</td>\n",
       "      <td>_×¥_¬¶¬½ùÏâó_ê_â_ñ¾óØ_¦_¶_</td>\n",
       "      <td>_öóÒâ__¬, Ï__ãóñ, __Ô¬__, _ù·_ó¦, ó...</td>\n",
       "      <td>8/4/20</td>\n",
       "      <td>8/4/20</td>\n",
       "      <td>11/13/20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://patents.google.com/patent/CN111925718A/en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nThe invention provides a universal primer fo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>universal primer nonferrous metal preparation ...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2264</th>\n",
       "      <td>JP-S58201860-A</td>\n",
       "      <td>Rust preventing coating material composition</td>\n",
       "      <td>Mitsui Toatsu Chem Inc, __¬¾óñ_ØÏ_×Ñ___óÒ¯_...</td>\n",
       "      <td>Yoshio Kikuta, _à­_êê___à , Toashi Kishi, ...</td>\n",
       "      <td>5/18/82</td>\n",
       "      <td>5/18/82</td>\n",
       "      <td>11/24/83</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://patents.google.com/patent/JPS58201860A/en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nPURPOSE:The titled novel composition consist...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>rust preventing coating material composition p...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2333</th>\n",
       "      <td>JP-S6072954-A</td>\n",
       "      <td>Long-period rustproof coating composition</td>\n",
       "      <td>Dainippon Toryo Co Ltd, __Ïó¥ïóØ¶_µ¥óÑ¦óÒ¯___...</td>\n",
       "      <td>Toshio Shinohara, Ò__ë_êêªà_íÆ, Toshimiki T...</td>\n",
       "      <td>9/30/83</td>\n",
       "      <td>9/30/83</td>\n",
       "      <td>4/25/85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://patents.google.com/patent/JPS6072954A/en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nPURPOSE:The titled composition which can for...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>long period rustproof coating composition purp...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2335</th>\n",
       "      <td>JP-S6078672-A</td>\n",
       "      <td>Corrosion-proof painting method of metal surfa...</td>\n",
       "      <td>Mitsui Eng &amp; Shipbuild Co Ltd, __¬¾_êÒ_óÒ¯...</td>\n",
       "      <td>Satoru Nishimoto, óë ï·óØ¶</td>\n",
       "      <td>10/4/83</td>\n",
       "      <td>10/4/83</td>\n",
       "      <td>5/4/85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://patents.google.com/patent/JPS6078672A/en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nPURPOSE:To apply corrosion-proof coating to ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>corrosion proof painting method metal surface ...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>CN-110157289-A</td>\n",
       "      <td>Watersoluble plumbago alkene anticorrosive paint</td>\n",
       "      <td>_ññ_Ø___óöë__·Ïâó_êóØ_¦_¶_</td>\n",
       "      <td>__Òó__, __óöï__</td>\n",
       "      <td>3/28/18</td>\n",
       "      <td>3/28/18</td>\n",
       "      <td>8/23/19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://patents.google.com/patent/CN110157289A/en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nThe present invention relates to a kind of w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>watersoluble plumbago alkene anticorrosive pai...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Master patent number                                              title  \\\n",
       "424        CN-111925718-A  Universal primer for nonferrous metal and prep...   \n",
       "2264       JP-S58201860-A      Rust preventing coating material composition    \n",
       "2333        JP-S6072954-A         Long-period rustproof coating composition    \n",
       "2335        JP-S6078672-A  Corrosion-proof painting method of metal surfa...   \n",
       "394        CN-110157289-A  Watersoluble plumbago alkene anticorrosive paint    \n",
       "\n",
       "                                               assignee  \\\n",
       "424                _×¥_¬¶¬½ùÏâó_ê_â_ñ¾óØ_¦_¶_   \n",
       "2264  Mitsui Toatsu Chem Inc, __¬¾óñ_ØÏ_×Ñ___óÒ¯_...   \n",
       "2333  Dainippon Toryo Co Ltd, __Ïó¥ïóØ¶_µ¥óÑ¦óÒ¯___...   \n",
       "2335  Mitsui Eng & Shipbuild Co Ltd, __¬¾_êÒ_óÒ¯...   \n",
       "394                _ññ_Ø___óöë__·Ïâó_êóØ_¦_¶_   \n",
       "\n",
       "                                        inventor/author priority date  \\\n",
       "424   _öóÒâ__¬, Ï__ãóñ, __Ô¬__, _ù·_ó¦, ó...        8/4/20   \n",
       "2264  Yoshio Kikuta, _à­_êê___à , Toashi Kishi, ...       5/18/82   \n",
       "2333  Toshio Shinohara, Ò__ë_êêªà_íÆ, Toshimiki T...       9/30/83   \n",
       "2335                       Satoru Nishimoto, óë ï·óØ¶       10/4/83   \n",
       "394                                __Òó__, __óöï__       3/28/18   \n",
       "\n",
       "     filing/creation date publication date grant date  \\\n",
       "424                8/4/20         11/13/20        NaN   \n",
       "2264              5/18/82         11/24/83        NaN   \n",
       "2333              9/30/83          4/25/85        NaN   \n",
       "2335              10/4/83           5/4/85        NaN   \n",
       "394               3/28/18          8/23/19        NaN   \n",
       "\n",
       "                                            result link  \\\n",
       "424   https://patents.google.com/patent/CN111925718A/en   \n",
       "2264  https://patents.google.com/patent/JPS58201860A/en   \n",
       "2333   https://patents.google.com/patent/JPS6072954A/en   \n",
       "2335   https://patents.google.com/patent/JPS6078672A/en   \n",
       "394   https://patents.google.com/patent/CN110157289A/en   \n",
       "\n",
       "     representative figure link  \\\n",
       "424                         NaN   \n",
       "2264                        NaN   \n",
       "2333                        NaN   \n",
       "2335                        NaN   \n",
       "394                         NaN   \n",
       "\n",
       "                                              Abstracts  Relevant  Round  \\\n",
       "424   \\nThe invention provides a universal primer fo...       NaN    NaN   \n",
       "2264  \\nPURPOSE:The titled novel composition consist...       5.0    1.0   \n",
       "2333  \\nPURPOSE:The titled composition which can for...       NaN    NaN   \n",
       "2335  \\nPURPOSE:To apply corrosion-proof coating to ...       NaN    NaN   \n",
       "394   \\nThe present invention relates to a kind of w...       NaN    NaN   \n",
       "\n",
       "                                               clean_TA  Uncertanty  \\\n",
       "424   universal primer nonferrous metal preparation ...        0.01   \n",
       "2264  rust preventing coating material composition p...        0.01   \n",
       "2333  long period rustproof coating composition purp...        0.01   \n",
       "2335  corrosion proof painting method metal surface ...        0.01   \n",
       "394   watersoluble plumbago alkene anticorrosive pai...        0.01   \n",
       "\n",
       "      Prediction score  \n",
       "424                100  \n",
       "2264               100  \n",
       "2333               100  \n",
       "2335               100  \n",
       "394                100  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict df using all 100 models\n",
    "random_numbers = [15, 32, 69, 75, 83, 8, 68, 99, 44, 92]\n",
    "#def predict_and_sort(all_data, tokenizer, random_numbers): \n",
    "predictions = []\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "targets = df['Relevant'].values\n",
    "for text in df['clean_TA']:\n",
    "    train_tokens = tokenizer.encode_plus(\n",
    "        text,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        #return_tensors='tf'\n",
    "        )\n",
    "    input_ids.append(train_tokens['input_ids'])\n",
    "    attention_masks.append(train_tokens['attention_mask'])\n",
    "features = np.array(input_ids)\n",
    "print('All data tokenized using tokenizer', tokenizer)\n",
    "\n",
    "for num in random_numbers:\n",
    "    for i in range(10):\n",
    "        model = keras.models.load_model(f'models/R{round}_model_{num}_{i}.keras')\n",
    "        predict = model.predict(features,batch_size=10)\n",
    "        predict = np.where(predict >= 0, 1, 0)\n",
    "        predictions.append(predict)\n",
    "\n",
    "sum_predictions = np.sum(predictions, axis=0)\n",
    "#calculate uncertainty as 1 divided by the sum of 1 predictions mininus the sum of 0 predictions\n",
    "#count of 0's in predictions\n",
    "count_0 = np.count_nonzero(np.array(predictions) == 0, axis=0)\n",
    "\n",
    "df[f'R{round} Uncertanty'] = 1 / (np.abs(sum_predictions - count_0)+10**(-6))\n",
    "df[f'R{round} Prediction score'] = sum_predictions\n",
    "df = df.sort_values(by=f'R{round} Prediction score', ascending=False)\n",
    "df.to_excel(f'R{round}_predictions_for_review.xlsx', index=False)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New code to remove the need for keras tuner\n",
    "### Keras Tuner is very nice, but it requiers saving the tuner which can make deployment more difficult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Define the model\n",
    "def create_model(learning_rate, num_units):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(num_units, activation='relu'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Define the hyperparameter ranges\n",
    "learning_rates = [0.01, 0.001, 0.0001, 0.00001]\n",
    "num_units_list = [32, 64, 128, 256, 512]\n",
    "\n",
    "# Function to randomly sample hyperparameters\n",
    "def sample_hyperparameters():\n",
    "    learning_rate = random.choice(learning_rates)\n",
    "    num_units = random.choice(num_units_list)\n",
    "    return learning_rate, num_units\n",
    "\n",
    "# Directory to save the best models\n",
    "save_dir = 'best_models'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Perform random search\n",
    "num_combinations = 50\n",
    "best_models = []\n",
    "\n",
    "for i in range(num_combinations):\n",
    "    learning_rate, num_units = sample_hyperparameters()\n",
    "    model = create_model(learning_rate, num_units)\n",
    "    model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val), verbose=0)\n",
    "    score = model.evaluate(X_val, y_val, verbose=0)\n",
    "    \n",
    "    # Save the model and its score\n",
    "    best_models.append((score, model, {'learning_rate': learning_rate, 'num_units': num_units}))\n",
    "    \n",
    "    # Keep only the 10 best models\n",
    "    best_models = sorted(best_models, key=lambda x: x[0])[:10]\n",
    "\n",
    "# Save the 10 best models\n",
    "for i, (score, model, params) in enumerate(best_models):\n",
    "    model_path = os.path.join(save_dir, f\"model_{i+1}.h5\")\n",
    "    model.save(model_path)\n",
    "    print(f\"Model {i+1}: Score = {score}, Hyperparameters = {params}, Saved at = {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandbox_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
